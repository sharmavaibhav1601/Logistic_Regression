{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical Questions\n",
        "\n",
        "---\n",
        "\n",
        "### **1. What is Logistic Regression, and how does it differ from Linear Regression?**  \n",
        "Logistic Regression is a classification algorithm that predicts probabilities using the sigmoid function. Unlike Linear Regression, which predicts continuous values, Logistic Regression outputs probabilities mapped to classes (0 or 1).  \n",
        "**Example:** Predicting if an email is spam (1) or not (0), whereas Linear Regression predicts house prices.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. What is the mathematical equation of Logistic Regression?**  \n",
        "The equation is:  \n",
        "\\[\n",
        "P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n)}}\n",
        "\\]  \n",
        "Where \\( P(Y=1|X) \\) is the probability of class 1, and \\( \\beta \\) are the model coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why do we use the Sigmoid function in Logistic Regression?**  \n",
        "The sigmoid function maps any real number to a range between 0 and 1, making it suitable for probability estimation. It helps classify inputs into two categories.  \n",
        "**Example:** A probability of 0.8 means the model is 80% confident the instance belongs to class 1.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. What is the cost function of Logistic Regression?**  \n",
        "Logistic Regression uses the **log loss (cross-entropy loss)**:  \n",
        "\\[\n",
        "J(\\theta) = - \\frac{1}{m} \\sum [ y \\log(h) + (1 - y) \\log(1 - h)]\n",
        "\\]  \n",
        "where \\( h \\) is the predicted probability. This function penalizes incorrect confident predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. What is Regularization in Logistic Regression? Why is it needed?**  \n",
        "Regularization (L1/L2) prevents overfitting by adding a penalty to large coefficients. It ensures the model generalizes well to unseen data.  \n",
        "**Example:** Without regularization, a model may perfectly fit training data but fail on new data.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Explain the difference between Lasso, Ridge, and Elastic Net regression.**  \n",
        "- **Lasso (L1):** Shrinks coefficients to zero, selecting important features.  \n",
        "- **Ridge (L2):** Reduces coefficient values but doesn’t eliminate them.  \n",
        "- **Elastic Net:** Combines L1 and L2, balancing feature selection and coefficient shrinkage.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. When should we use Elastic Net instead of Lasso or Ridge?**  \n",
        "Use **Elastic Net** when data has many correlated features. Lasso alone might randomly select one feature, while Elastic Net balances feature selection and shrinkage, improving stability.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. What is the impact of the regularization parameter (λ) in Logistic Regression?**  \n",
        "- **High \\( \\lambda \\):** Stronger regularization, smaller coefficients, avoids overfitting.  \n",
        "- **Low \\( \\lambda \\):** Weak regularization, fits training data better but may overfit.  \n",
        "- **Zero \\( \\lambda \\):** No regularization, behaving like standard Logistic Regression.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. What are the key assumptions of Logistic Regression?**  \n",
        "1. **Linearity in log-odds:** The relationship between predictors and log-odds is linear.  \n",
        "2. **No multicollinearity:** Independent variables should not be highly correlated.  \n",
        "3. **Large sample size:** More data improves model stability.  \n",
        "4. **Independent observations:** No autocorrelation in data.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. What are some alternatives to Logistic Regression for classification tasks?**  \n",
        "1. **Decision Trees**  \n",
        "2. **Random Forest**  \n",
        "3. **Support Vector Machines (SVM)**  \n",
        "4. **Naïve Bayes**  \n",
        "5. **Neural Networks**  \n",
        "6. **Gradient Boosting (XGBoost, LightGBM, CatBoost)**  \n",
        "\n",
        "---\n",
        "\n",
        "### **11. What are Classification Evaluation Metrics?**  \n",
        "- **Accuracy:** Correct predictions/Total predictions.  \n",
        "- **Precision:** \\( TP / (TP + FP) \\).  \n",
        "- **Recall:** \\( TP / (TP + FN) \\).  \n",
        "- **F1-score:** Harmonic mean of Precision and Recall.  \n",
        "- **ROC-AUC:** Measures classifier performance across different thresholds.\n",
        "\n",
        "---\n",
        "\n",
        "### **12. How does class imbalance affect Logistic Regression?**  \n",
        "Class imbalance skews probability estimates, making the model biased toward the majority class. **Solutions:**  \n",
        "- Use **balanced class weights**.  \n",
        "- **Oversample the minority class** (SMOTE).  \n",
        "- **Undersample the majority class**.  \n",
        "- Use different **evaluation metrics** like F1-score and AUC.\n",
        "\n",
        "---\n",
        "\n",
        "### **13. What is Hyperparameter Tuning in Logistic Regression?**  \n",
        "Hyperparameter tuning optimizes model performance by adjusting parameters like:  \n",
        "- **Regularization strength (\\(\\lambda\\))**  \n",
        "- **Solver choice (liblinear, saga, lbfgs, newton-cg)**  \n",
        "- **Class weights**  \n",
        "Use **GridSearchCV** or **RandomizedSearchCV** to find the best values.\n",
        "\n",
        "---\n",
        "\n",
        "### **14. What are different solvers in Logistic Regression? Which one should be used?**  \n",
        "- **liblinear:** Good for small datasets.  \n",
        "- **lbfgs & newton-cg:** Best for multiclass problems.  \n",
        "- **saga:** Works well with large datasets and L1/L2 regularization.  \n",
        "\n",
        "Use **liblinear** for binary and **lbfgs** for multiclass.\n",
        "\n",
        "---\n",
        "\n",
        "### **15. How is Logistic Regression extended for multiclass classification?**  \n",
        "Two common approaches:  \n",
        "- **One-vs-Rest (OvR):** Trains one classifier per class.  \n",
        "- **Softmax Regression:** Uses a single model to assign probabilities to multiple classes.  \n",
        "**Example:** Classifying handwritten digits (0-9).\n",
        "\n",
        "---\n",
        "\n",
        "### **16. What are the advantages and disadvantages of Logistic Regression?**  \n",
        "\n",
        "**Advantages:**  \n",
        "✅ Simple and easy to interpret.  \n",
        "✅ Works well with small datasets.  \n",
        "✅ Probabilistic output.  \n",
        "\n",
        "**Disadvantages:**  \n",
        "❌ Assumes linear decision boundary.  \n",
        "❌ Struggles with high-dimensional data.  \n",
        "❌ Sensitive to multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "### **17. What are some use cases of Logistic Regression?**  \n",
        "- **Medical Diagnosis:** Predicting disease risk (e.g., diabetes).  \n",
        "- **Fraud Detection:** Identifying fraudulent transactions.  \n",
        "- **Marketing:** Predicting customer churn.  \n",
        "- **HR Analytics:** Employee attrition prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### **18. What is the difference between Softmax Regression and Logistic Regression?**  \n",
        "- **Logistic Regression:** Used for **binary classification** (0 or 1).  \n",
        "- **Softmax Regression:** Extends Logistic Regression for **multiclass problems** by normalizing probabilities across multiple classes.\n",
        "\n",
        "---\n",
        "\n",
        "### **19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**  \n",
        "- **OvR:** Preferred when the dataset is small and training time is a concern.  \n",
        "- **Softmax:** Works better for **mutually exclusive** classes and large datasets.  \n",
        "\n",
        "---\n",
        "\n",
        "### **20. How do we interpret coefficients in Logistic Regression?**  \n",
        "Each coefficient represents the change in log-odds of the outcome for a one-unit increase in the predictor, keeping others constant.  \n",
        "**Example:** If \\( \\beta_1 = 0.5 \\), increasing \\( X_1 \\) by 1 increases the odds of success by \\( e^{0.5} \\).  \n",
        "\n"
      ],
      "metadata": {
        "id": "V6U99L0WuplM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Z7Yj2ofywr79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with L1 regularization\n",
        "log_reg_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "log_reg_l1.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = log_reg_l1.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Z61g_AjYw0fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "log_reg_l2 = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
        "log_reg_l2.fit(X_train, y_train)\n",
        "\n",
        "y_pred = log_reg_l2.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy:.4f}\")\n",
        "print(\"Coefficients:\", log_reg_l2.coef_)\n"
      ],
      "metadata": {
        "id": "UJmJKA2jw7Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "log_reg_en = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "log_reg_en.fit(X_train, y_train)\n",
        "\n",
        "y_pred = log_reg_en.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "YhIoBoooxARc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "log_reg_ovr = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "log_reg_ovr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = log_reg_ovr.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy using One-vs-Rest (OvR): {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "k05amBcGxF5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=200), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")\n"
      ],
      "metadata": {
        "id": "f6X_UxCcxO_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "scores = cross_val_score(log_reg, X, y, cv=skf, scoring='accuracy')\n",
        "print(f\"Average Accuracy: {scores.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "10zbYy0-xUpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Replace 'your_dataset.csv' with the actual path to your dataset\n",
        "# If the file is in the same directory as the notebook, just use the filename\n",
        "# Example: df = pd.read_csv('data.csv')\n",
        "# or provide the full path if in a different directory.\n",
        "# Example: df = pd.read_csv('/path/to/your/dataset.csv')\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "X = df.drop(columns=['target'])  # Assuming 'target' is your target variable column\n",
        "y = df['target']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)  # Increase max_iter if needed\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Z03VbKh-1iTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. M Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga']}\n",
        "rand_search = RandomizedSearchCV(LogisticRegression(max_iter=200), param_dist, cv=5, scoring='accuracy', n_iter=10)\n",
        "rand_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", rand_search.best_params_)\n",
        "print(f\"Best Accuracy: {rand_search.best_score_:.4f}\")\n"
      ],
      "metadata": {
        "id": "XqRPDKbIxeP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "log_reg_ovo = LogisticRegression(multi_class='ovo', solver='lbfgs', max_iter=200)\n",
        "log_reg_ovo.fit(X_train, y_train)\n",
        "\n",
        "y_pred = log_reg_ovo.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy using One-vs-One (OvO): {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "jbQ0ssdRxjRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9GhjYcOyxlnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "oacEBczAxqM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "log_reg_balanced = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "log_reg_balanced.fit(X_train, y_train)\n",
        "\n",
        "y_pred = log_reg_balanced.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Balanced Class Weights: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "7EkBGXDSxvfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "titanic = pd.read_csv(\"titanic.csv\")\n",
        "titanic.fillna(titanic.mean(), inplace=True)  # Fill missing values\n",
        "X = titanic.drop(columns=['Survived'])\n",
        "y = titanic['Survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy_score(y_test, log_reg.predict(X_test)):.4f}\")\n"
      ],
      "metadata": {
        "id": "qkwvTZN1x4lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "log_reg_scaled = LogisticRegression(max_iter=200)\n",
        "log_reg_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_scaled = log_reg_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without Scaling: {accuracy:.4f}\")\n",
        "print(f\"Accuracy with Scaling: {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "id": "9b4w6vNDx9lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Ensure log_reg is trained on the correct data (Titanic in this case)\n",
        "log_reg.fit(X_train, y_train)  # Fit the model to the Titanic data\n",
        "\n",
        "# Get probabilities for all classes (2D array)\n",
        "y_prob = log_reg.predict_proba(X_test)\n",
        "\n",
        "# Use roc_auc_score for binary classification without multi_class parameter\n",
        "roc_auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qvQm13K44Dun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "log_reg_custom = LogisticRegression(C=0.5, max_iter=200)\n",
        "log_reg_custom.fit(X_train, y_train)\n",
        "\n",
        "y_pred_custom = log_reg_custom.predict(X_test)\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "\n",
        "print(f\"Model Accuracy with C=0.5: {accuracy_custom:.4f}\")\n"
      ],
      "metadata": {
        "id": "sNst33lHyJXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "feature_importance = pd.Series(log_reg.coef_[0], index=X.columns)\n",
        "feature_importance.sort_values(ascending=False, inplace=True)\n",
        "\n",
        "print(\"Top Important Features:\")\n",
        "print(feature_importance)\n"
      ],
      "metadata": {
        "id": "cNqbid-EyOVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen’s Kappa Score: {kappa_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "-TWnhn53yTII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test and y_prob are from a multiclass problem\n",
        "# Choose one class to treat as positive (e.g., class 1)\n",
        "pos_class = 1\n",
        "\n",
        "# Convert y_test to binary for the chosen class\n",
        "y_test_binary = np.where(y_test == pos_class, 1, 0)\n",
        "\n",
        "# Get probabilities for the positive class only\n",
        "y_prob_binary = y_prob[:, pos_class]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test_binary, y_prob_binary)\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Class {}'.format(pos_class))\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "vVwHBPqx4jbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    print(f\"Accuracy with solver {solver}: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "j_xoLNeIycCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n"
      ],
      "metadata": {
        "id": "Hi0u5SJxyme7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# 23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "# Import necessary libraries if not already imported\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the fitted scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train the Logistic Regression model on scaled data\n",
        "log_reg_scaled = LogisticRegression(max_iter=200)\n",
        "log_reg_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Now you can use log_reg_scaled for prediction\n",
        "acc_scaled = accuracy_score(y_test, log_reg_scaled.predict(X_test_scaled))"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UI92A5Fo5AIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "C_values = [0.01, 0.1, 1, 10]\n",
        "for C in C_values:\n",
        "    model = LogisticRegression(C=C, max_iter=200)\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "    print(f\"C={C}, Mean Accuracy: {scores.mean():.4f}\")\n"
      ],
      "metadata": {
        "id": "hEct9izpy1i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "import joblib\n",
        "\n",
        "# Train and save model\n",
        "joblib.dump(log_reg, 'logistic_model.pkl')\n",
        "\n",
        "# Load model\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "\n",
        "# Make predictions\n",
        "y_pred_loaded = loaded_model.predict(X_test)\n",
        "print(f\"Accuracy of Loaded Model: {accuracy_score(y_test, y_pred_loaded):.4f}\")\n"
      ],
      "metadata": {
        "id": "ScQ06mLby8Bc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}